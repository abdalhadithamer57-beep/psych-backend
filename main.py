import os
import sqlite3
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI

load_dotenv()
app = FastAPI()

# âœ… Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø¨Ø§Ù„Ø§ØªØµØ§Ù„ Ø¹Ø¨Ø± Nginx
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def get_db_connection():
    conn = sqlite3.connect('psych_consultant.db', check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    conn = get_db_connection()
    conn.execute('''CREATE TABLE IF NOT EXISTS messages 
                    (user_id TEXT, role TEXT, content TEXT, timestamp DATETIME DEFAULT CURRENT_TIMESTAMP)''')
    conn.commit()
    conn.close()

init_db()

# --- Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« RAG ---
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

def build_vector_db():
    path = "./knowledge_base/"
    if not os.path.exists(path): 
        os.makedirs(path)
        return None
    loader = PyPDFDirectoryLoader(path)
    documents = loader.load()
    if not documents: return None
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    splits = text_splitter.split_documents(documents)
    return FAISS.from_documents(splits, embeddings)

db = build_vector_db()

@app.get("/")
async def root():
    return {"status": "online", "message": "Psych Consultant API is running"}

@app.post("/chat")
async def chat_endpoint(request: Request):
    try:
        data = await request.json()
        user_query = data.get("message", "")
        user_id = data.get("user_id", "guest_user") 

        # 1. Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ù…Ø§ÙŠØ©
        safety_words = ["Ø§Ù†ØªØ­Ø§Ø±", "Ø¥ÙŠØ°Ø§Ø¡", "Ø£Ù‚ØªÙ„ Ù†ÙØ³ÙŠ", "Ø§Ù†ØªØ­Ø±", "Ø£Ù†Ù‡ÙŠ Ø­ÙŠØ§ØªÙŠ"]
        if any(word in user_query for word in safety_words):
            return {
                "response": "Ø£Ù†Ø§ Ø£Ù‡ØªÙ… Ù„Ø£Ù…Ø±Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ù…Ø®ØªØµÙŠÙ† ÙÙˆØ±Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ 911.", 
                "source": "ğŸš¨ Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ù…Ø§ÙŠØ©",
                "isEmergency": True
            }

        # 2. Ø­ÙØ¸ Ø§Ù„Ø³ÙŠØ§Ù‚
        conn = get_db_connection()
        conn.execute("INSERT INTO messages (user_id, role, content) VALUES (?, ?, ?)", (user_id, "user", user_query))
        conn.commit()

        # 3. Ø¬Ù„Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        past_msgs = conn.execute("SELECT role, content FROM messages WHERE user_id = ? ORDER BY rowid DESC LIMIT 6", (user_id,)).fetchall()
        memory_text = "\n".join([f"{'Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' if m['role']=='user' else 'Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±'}: {m['content']}" for m in reversed(past_msgs)])

        # 4. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª
        context = ""
        if db:
            docs = db.similarity_search(user_query, k=3)
            context = "\n".join([d.page_content for d in docs])

        system_instruction = f"Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù†ÙØ³ÙŠ Ø¯Ø§ÙØ¦. Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:\n{memory_text}\n\nÙ…Ø±Ø§Ø¬Ø¹ Ø¹Ù„Ù…ÙŠØ©:\n{context}"
        
        # 5. Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Groq -> OpenAI)
        final_response = ""
        badge = "Ø§Ø³ØªØ´Ø§Ø±Ø© Ø°ÙƒÙŠØ©"
        try:
            llm = ChatGroq(temperature=0.4, model_name="llama-3.1-8b-instant", groq_api_key=os.getenv("GROQ_API_KEY"))
            final_response = llm.invoke([{"role": "system", "content": system_instruction}, {"role": "user", "content": user_query}]).content
        except:
            llm_backup = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key=os.getenv("OPENAI_API_KEY"))
            final_response = llm_backup.invoke([{"role": "system", "content": system_instruction}, {"role": "user", "content": user_query}]).content
            badge = "Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"

        conn.execute("INSERT INTO messages (user_id, role, content) VALUES (?, ?, ?)", (user_id, "assistant", final_response))
        conn.commit()
        conn.close()

        return {"response": final_response, "source": badge, "isEmergency": False}
    except Exception as e:
        return {"response": "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙ†ÙŠ.", "error": str(e)}
